---
title: "Expected credit loss"
author: "Eligijus Bujokas"
date: "01/07/2021"
fontsize: 8pt
output: 
  beamer_presentation:
    theme: "Singapore"
    number_sections: true
   #transition: slower
   #logo: presentation/scorify.jpg
   #widescreen: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr)    # For knitting document and include_graphics function
library(ggplot2)  # For plotting
library(png)
library(data.table)
library(stringr)
library(reshape)
library(gridExtra)
library(pROC)
library(dplyr)
library(ROCR)
library(betareg)
Sys.setlocale("LC_ALL","C")
```

## Aim of this presentation

The aim of this presentation is to model and present the various techniques of working with expected credit losses using real life data.

The subject in this presentation will be one of the biggest short term loan lenderer in US: **Lending Club**.

```{r out.width="75%", fig.align='center'}
include_graphics("presentation/lending-club.png")
```

## Acknowledgment

All the logic and models are presented in the course https://www.udemy.com/course/credit-risk-modeling-in-python

The presenter of the material is Nikolay G. Georgiev, PhD from the Norwegian Business School.

The data is taken from kaggle https://www.kaggle.com/ethon0426/lending-club-20072020q1

Lending club website https://www.lendingclub.com/

## Expected credit loss 

$$ \mathbb E[CL] = \mathbb E[PD] \mathbb E[LGD] \mathbb E[EAD]  $$
CL - Credit losses

PD - Probability of default 

LGD - Losses given default

EAD - Exposure at default

## Expected credit loss modeling in a nutshell

```{r out.width="75%", fig.align='center'}
include_graphics("presentation/modeling.png")
```

## Exploring the data 

The data is monthly spanning from 2008 January up until 2018 December (included).

```{r include=FALSE}
# Reading train and test data 
train <- fread('data/train.csv')
test <- fread('data/test.csv')

d <- rbind(train, test)

# Creating the formated column
d[, issue_d_formated:=zoo::as.yearmon(issue_d_formated, "%b %Y")]

# Saving the Ypd as Y for ploting purposes
d$Y <- d$Ypd

# Aggregating by issue date and sorting
loan_n = d[, .('loans_monthly' = .N), by="issue_d_formated"] %>% 
  .[order(issue_d_formated)]
```

```{r}
# Ploting the time series
ggplot(data=loan_n, aes(x=issue_d_formated, y=loans_monthly)) +
  geom_bar(stat="identity", fill="steelblue", color='black') + 
  ggtitle('Monthly count of issued loans') + 
  labs(x='month', y='loan count')
```

## Loan status 

There is a feature in the data called **loan_status**. The values can be: 


```{r, comment=NA}
d[, .N, by="loan_status"] %>% 
  .[order(N)] %>% 
  kable(.)
```

## Bad loans

```{r, warning=NA, comment=NA}
# Visualizing the distribution of Y 
bymonth <- d[, .N, by=c("issue_d_formated", "Ypd")] %>% 
  cast(., issue_d_formated ~ Ypd, value="N") %>% 
  data.table(.) %>% 
  .[, rate:=bad * 100/(bad + good)]

# Calculating the shares
ggplot(data=bymonth, aes(x=issue_d_formated, y=rate)) + 
  geom_area(color="darkblue", fill="lightblue") + 
  labs(x='month', y='default_rate') + 
  ggtitle('Default rate per month')
```

## Distribution by grade

```{r}
share_by_grade = d[, .("total_issued_grade" = .N), by=c('issue_d_formated', 'grade')]
total_issued = d[, .("total_issued" = .N), by='issue_d_formated']

# Merging the information 
merged = merge(share_by_grade, total_issued, all.x=TRUE)

# Calculating the share
merged[, share:=total_issued_grade/total_issued]

ggplot(data=merged, aes(x=issue_d_formated, y=share, fill=grade)) + geom_bar(position="stack", stat="identity", color='steelblue')
```

## The Basel Accords

The Basel II accord, which was signed in 2004, defined three strict guidelines:

* How much capital banks need to have

* How capital is defined

* How capital is compared against risk-weighted assets

One of the main takeouts from both the basel II and subsequent basel III accords is that 

**The greater the risk a bank is exposed to, the greater the amount of capital it needs to hold**


## Probability of default (PD)

This is the most strict part of the three components of ECL and must follow certain rules in modeling. Every feature, both categorical and numeric, needs to be transformed into dummy variables.

To infer what categorical feature values are the most influential in determining bad loan from a good loan we can use the weight of evidence (WOE for short) criteria. For a feature $i$ and the feature level $j$ the $WOE_{i, j}$ is calculated with the following formula:

$$WOE_{i, j} = log \left(\dfrac{P(X_{i}=j|Y=1)}{P(X_{i} = j|Y=0)}\right)$$

## WOE example

```{r woe, include=FALSE}
woe <- function(feature, data){
  # Grouping by the feature and getting the total number of observations in each level
  woeTable <- data[, .N, by=c("Y", feature)] %>% 
    # Seting the names 
    setnames(., c('Y', 'feature', 'N')) %>% 
    # Ommiting NA
    na.omit(.) %>% 
    # Casting so that Y is in the columns
    cast(., feature ~ Y, value="N") %>% 
    # Converting back to data.table
    data.table(.) %>% 
    # Filling the missing values with 0 
    setnafill(., fill=0, cols=c('good', 'bad')) %>%
    # Getting the proportions of good and bad borrowers in each level of the variable
    .[, c('prop_good', 'prop_bad') := list(good/sum(good), bad/sum(bad))] %>% 
    # Calculating the WOE statistic 
    .[, woe:=log(prop_good/prop_bad)] %>% 
    # Sorting by the woe stat
    .[order(woe, decreasing=TRUE)]
  
  return(woeTable)
}

plotWoe <- function(ft, woeData){
  p <- ggplot(woeData, aes(x=reorder(feature, woe), y=woe)) + 
            geom_bar(stat="identity", fill="steelblue", color='black') + 
            ggtitle(ft) + 
            labs(x='Feature value') +
            coord_flip()
   return(p)
}
```

```{r}
woedf <- woe('term', d)
kable(woedf)
```

## WOE for interest rates

```{r}
woedf <- woe('int_rateg', d)
plotWoe('Interest rate groups', woedf)
```

## Annual income 

```{r}
woedf <- woe('annual_incg', d)
plotWoe('Annual income splits', woedf)
```

## Some more features

```{r some more features}
cat_vars <- c( "grade", "month_diffg", 'dtig', 'pub_recg')
plots <- list()

for(cat_var in cat_vars){
  woeStat <- woe(cat_var, d)
  plots[[cat_var]] <- plotWoe(cat_var, woeStat)
}

grid.arrange(plots$grade, plots$month_diff, plots$dti, plots$pub_rec, ncol = 2, nrow=2)
```

## Final variable list used for PD

```{r features, comment=NA, warning=NA}

featuresUsed <- c(
  "term",
  "pub_rec",
  "annual_inc",
  "dti",
  "month_diff",
  "emp_length",
  "int_rate",
  "loan_amnt",
  "grade",
  "purpose",
  "addr_state"
)
kable(featuresUsed)
```


## Machine learning method 

Now that we have our $X$ matrix and our $Y$ matrix, we need a method to model the relationship between them. 

A popular choice is the **logistic regression** model for binary classification.

We want to estimate the following conditional probability:

$$P(Y = 1|X)$$

In our case:

$$P(Y = good\_loan| data)$$

## Logistic regression - regression part  

**Regress** - "coming back to" (liet. - grįžimas). 

The term is accredited to Francis Galton in the 19th century in his biological work. 

**Regression** $\approx$ "coming back to the mean"

Regression models try to model the expected value (average) of the dependent variable with the independent ones. 

In general terms: 

$$\mathbb E[Y|X] = \mu = g^{-1}(\beta_{0} + \Sigma_{i=1}^{k} \left( \beta_{i} X_{i} \right)) $$

$g(.)$ is called the link function.

## Logistic regression - logistic part

The standard logistic function is: 

$$ logistic(x) = \dfrac{1}{1 + e^{-x}} $$

$$ logistic : (-\infty, +\infty) \rightarrow (0, 1) $$

The logit (log-odds) function is: 

$$logit(x) = log(\dfrac{x}{1-x})$$

$$logit: (0, 1) \rightarrow (-\infty, +\infty)$$

$$logit^{-1}(x) = logistic(x)$$

## Logistic regression equation 

Putting "logistic" and "regression" together: 

Lets define:

$$z := \beta_{0} + \Sigma_{i=1}^{k} \left( \beta_{i} X_{i} \right)) $$

Logistic regression is form of general linear models (GLM) where the link function is the logit function. 

$$ \mathbb E[Y|X] = P(Y=1|X) = logit^{-1}(z) = logistic(z) = \dfrac{1}{1 + e^{-z}}$$

## Bernouli distribution 

Because our Y variable is either 1 or 0 (good or bad loan) we should talk about the Bernoulli distribution. 

$$ Y \in \{0, 1\} $$

$$P(Y=1) = p = 1 - P(Y=0)$$

The distribution is:

$$ f(y, p) = p ^ {y} (1 - p)^ {1 - y}  $$

## Bernouli and logistic regression maximum likelihood

The likelihood function (likelihood) measures the goodness of fit of a statistical model to a sample of data for **given values of the unknown parameters**. 

$$ l(\theta | x) = p_{\theta}(x) = P_{\theta}(X=x)$$

The maximum likelihood is a method to maximize the likelihood function in terms of the parameter $\theta$ such that the observed data is most probable given the assumption about the distribution of the data.

Bernoulli:

$$ l(p) = \Pi_{i=1}^{n} p^{y_{i}}(1 - p)^{1 - y_{i}}  $$

Logistic:

$$ l(\beta) = \Pi_{i=1}^{n} \left[ logistic(z) ^ {y_{i}} ((1 - logistic(z))^{1 - y_{i}})  \right] $$

## Estimating the coefficients


$$L(\beta| Y, X) = log(l ( \beta | Y, X)) $$

$$ L(\beta) = \Sigma_{i=1}^{n}\left[y_{i} log(logisitc(z)) + (1 - y_{i})log(1 - logistic(z))  \right] $$ 

The computer tries to find the "best" $\beta$ values such that the probability is as big as possible for witnessing the Y in our sample from the given X.

## The model for PD 

```{r} 
PDcoefs <- fread('modelPD.csv')
intercept <- as.numeric(PDcoefs[1][, 'coef'])
ggplot(PDcoefs, aes(y=feature, x=coef)) + geom_bar(stat="identity", col='steelblue', fill='white')
```

## Results on the test set

```{r warning=NA, comment=NA}
testPD <- fread("PDresults.csv")
#par(mfrow=c(1, 3))
pred <- ROCR::prediction(testPD$yhat, testPD$Y)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")

ROC <- roc(testPD$Y, testPD$yhat)
roc_stat <- ROC$auc

plot(perf, main=paste('ROC plot - GINI:', round(2 * roc_stat - 1, 3)))
abline(b=1, a=0, col='blue')

#perf <- performance(pred, measure = "prec", x.measure = "cutoff")
#plot(perf, main='Precision plot')

#perf <- performance(pred, measure = "rec", x.measure = "cutoff")
#plot(perf, main='Recall plot')
```

## Converting to a scorecard

We will recalibrate the coefficients to be between 200 and 800.

```{r}
refCols <- c(
  "term:60 months", 
  "annual_incg:[0,30)",
  "dtig:30+",
  "emp_lengthg:< 1 year",
  "int_rateg:25+",
  "grade:G",
  "loan_amntg:22000+",
  "pub_recg:>0",
  "month_diffg:[0, 70)",
  "addr_stateg:OK:AL:AR:MS:NE",
  "purposeg:small_business"
  )

coefFrame <- PDcoefs %>% select(c('feature', 'coef'))
coefFrameRef <- data.table(feature=refCols, coef=0)

coefFrame <- rbind(coefFrame, coefFrameRef)

# Removing the ` symbol from the feature names 
coefFrame$feature <- gsub('`', '', coefFrame$feature)

# Getting the original name 
coefFrame[, c('origFeature') := tstrsplit(feature, ":")[[1]]]

# Defining the min and max score
min_score <- 200
max_score <- 800

# Getting the lowest possible scores from each feature
mincoef <- coefFrame[, .("coef_value"=min(coef)), by='origFeature']
maxcoef <- coefFrame[, .("coef_value"=max(coef)), by='origFeature']

minmaxframe <- merge(mincoef, maxcoef, by='origFeature') %>% 
  setnames(., c('coef_value.x', 'coef_value.y'), c("min_coef", 'max_coef')) 

kable(minmaxframe)
```

## Formulas for converting 

To convert the coefficient to a score, we need to follow the following formula:

$$score = coef\dfrac{(max\_score - min\_score)}{maxsum\_coef - minsum\_coef}$$

To adjust the coefficient for the intercept we will use the formula:

$$ score\_intercept = \dfrac{(intercept\_coef - min\_sum\_coef)}{(max\_sum\_coef - min\_sum\_coef)} (max\_score - min\_score) + min\_score $$

## Final scorecard

```{r}

# Summing all the values and adding the intercept to get the lowest and the biggest possible scores
sumMin <- sum(mincoef$coef_value)
sumMax <- sum(maxcoef$coef_value)

coefFrame$score <- coefFrame$coef * (max_score - min_score) / (sumMax - sumMin)

scoreIntercept <- min_score + (max_score - min_score) * ((intercept - sumMin)/(sumMax - sumMin))

# Adding the value to the scorecard
coefFrame[feature=='(Intercept)']$score <- scoreIntercept

# Rounding the scores
coefFrame$score_rounded <- sapply(coefFrame$score, round)

# Getting the differences 
coefFrame$diff <- coefFrame$score - coefFrame$score_rounded

# Getting the maximum score from the scorecard 
maxScoreCard <- coefFrame[, .('maxscore'=max(score_rounded)),  by='origFeature']
maxScoreCard <- sum(maxScoreCard$maxscore)
diff <- max_score - maxScoreCard

# Adding the difference to the biggest rounded group
coefFrame[diff==min(diff)]$score_rounded <- coefFrame[diff==min(diff)]$score_rounded + diff

# Getting the minum rounded error
minScoreCard <- coefFrame[, .('minscore'=min(score_rounded)),  by='origFeature']
minScoreCard <- sum(minScoreCard$minscore)
diff <- min_score - minScoreCard

# Adding the difference to the biggest rounded group
coefFrame[diff==max(diff)]$score_rounded <- coefFrame[diff==max(diff)]$score_rounded + diff

scorecard <- coefFrame[order(-score_rounded)] %>% 
  .[, c('feature', 'score_rounded')] %>% 
  .[feature!='(Intercept)'] 

ggplot(scorecard, aes(y=reorder(feature, score_rounded), x=score_rounded)) + 
  geom_bar(stat="identity", col='steelblue', fill='white') + 
  labs(x='Score', y='feature')
```

## EAD modeling - Y variable

The dependent variable for the exposure at default is the amount of funds that a bank is at risk at a default event. They way we model it is using a variable called 
credit conversion factor (CCF):

$$ CCF_{i} = \dfrac{funded\_amount_{i} - received\_payments_{i}}{funded\_amount_{i}} $$

The higher the CCF for a given loan, the bigger is the EAD sum:

$$ EAD_{i} = CCF_{i} * funded\_amount_{i} $$

## EAD Y variable distribution

```{r warning=NA, comment=NA}
trainEAD <- fread('data/train_defaults.csv')
testEAD <- fread('data/test_defaults.csv')

trainEAD <- trainEAD[(CCF < 1) & (CCF>0)]
ggplot(data=trainEAD, aes(x=CCF)) + geom_histogram(fill='steelblue', color='black') + ggtitle('CCF')
```

## Beta regression 

When the dependant variable is a ratio and distributed similar to a beta distribution, we can use the beta regression method. 

The density for a beta distributed variable can be defined as:

$$ f(y; \mu, \phi) = \dfrac{\Gamma(\phi)}{\Gamma(\mu \phi)\Gamma((1-\mu)\phi)} y^{\mu\phi-1}(1 - y)^{(1 - \mu)\phi-1}$$

$0 < y < 1$ 

$0 < \mu < 1$

$\phi > 0$

$$ \mu = g^{-1}(X\beta) $$

## Beta regression - estimation

Same as in logistic regression - maximizing likelihood:

$$ l(\mu, \phi) = log(\Gamma(\phi)) - log(\Gamma(\phi \mu))  - log(\Gamma((1 - \mu)\phi)) + (\mu\phi -1)log(y) + ((1 - \mu)\phi - 1)log(1 - y)$$

## The model for EAD 

```{r}
EADcoefs <- fread('modelEAD.csv')
EADcoefs$exp_beta <- exp(EADcoefs$coef)
kable(EADcoefs)
```

## EAD residuals in the test set 

```{r warning=NA, comment=NA}
EADresults <- fread("EADresults.csv") %>% 
  select(-"V1")

# Getting the residuals 
EADresults$res <- EADresults$CCF - EADresults$CCF_hat

maeEAD <- sum(abs(EADresults$res)) / nrow(EADresults)
p1 <- ggplot(data=EADresults, aes(x=res)) + geom_histogram(fill='steelblue', color='black') + 
  ggtitle(paste('MAE: ', round(maeEAD, 3)))

p2 <- ggplot(EADresults, aes(x=CCF, y=res)) + geom_point(col='steelblue', alpha=0.4)

grid.arrange(p1, p2)
```

## LGD modeling - Y variable

LGD stands for losses given default. When dealing with loan data we can model recovery rate. Then, for each loan,

$$ LGD = 1 - recovery\_rate $$
In this data set, we calculate the recovery rate using the following equation:

$$ rt_{i} = \dfrac{recoveries_{i}}{funded\_amount_{i}} $$

$i$ - loan $i$.

When modeling the rt in this dataset we need to take into account only those loans who were charged off.

## LGD Y variable distribution

```{r warning=NA, comment=NA}
trainLGD <- fread('data/train_defaults.csv')
testLGD <- fread('data/test_defaults.csv')

trainLGD <- trainLGD[(recovery_rate < 1) & (recovery_rate>0)]
ggplot(data=trainLGD, aes(x=recovery_rate)) + geom_histogram(fill='steelblue', color='black') + ggtitle('Recovery rates')
```

## The model for LGD 

```{r}
LGDcoefs <- fread('modelLGD.csv') %>% select(-'V1')
kable(LGDcoefs)
```

## LGD residuals in the test set

```{r comment=NA, warning=NA}
LGDresults <- fread("LGDresults.csv")

# Getting the residuals 
LGDresults$res <- LGDresults$recovery_rate - LGDresults$recovery_rate_hat

maeLGD <- sum(abs(LGDresults$res)) / nrow(LGDresults)
p1 <- ggplot(data=LGDresults, aes(x=res)) + geom_histogram(fill='steelblue', color='black') + 
  ggtitle(paste('MAE: ', round(maeLGD, 3)))

p2 <- ggplot(LGDresults, aes(x=recovery_rate, y=res)) + geom_point(col='steelblue', alpha=0.4)

grid.arrange(p1, p2)
```

## Combining all the parts together 

The final ECL estimate using the dependent variables we estimated: 

$$ ECL =(1 - P(Y=1)) * (1 - recovery\_rate) * CCF * loan\_amount $$

## Distribution of expected credit loss

```{r comment=NA, warning=NA}
EAD <- fread('test-all-EAD.csv')
LGD <- fread('test-all-LGD.csv')
PD <- fread('test-all-PD.csv')

# Saving the results
test$CCF_hat <- EAD$CCF_hat
test$recovery_rate_hat <- LGD$rt_hat
test$PD_hat <- PD$yhat

# Getting the predicted percentage lost 
test$LGD_rate <- 1 - test$recovery_rate_hat

# Calculating the ECL for each loan 
test$ECL <- (1 - test$PD_hat) * test$LGD_rate * test$CCF_hat * test$loan_amnt

ggplot(test, aes(x=ECL)) + geom_histogram(col='steelblue', fill='lightblue') + ggtitle(paste("Mean loss: ", round(mean(test$ECL), 2)))

test$lost_amount <- test$loan_amnt * (1 - test$recovery_rate) * test$CCF * ifelse(test$Ypd=='bad', 1, 0)

test$res <- abs(test$ECL - test$lost_amount)
```

## Absolute residual distribution

```{r comment=NA, warning=NA}
ggplot(test, aes(x=res)) + geom_histogram(col='steelblue', fill='lightblue') + ggtitle(paste("Median: ", round(median(test$res), 2)))
```

## Final results

```{r}
results <- data.frame('total_loss'=sum(test$lost_amount) / 10^6, 'ECL'=sum(test$ECL) / 10^6, 'total_funded'=sum(test$loan_amnt) / 10^6)
results$amnt_default <- results$total_loss/results$total_funded
results$amnt_default_fc <- results$ECL / results$total_funded

kable(results)
```
